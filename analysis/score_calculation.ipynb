{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomo-for-lab/Code-of-automating-DE/blob/main/analysis/score_calculation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "frrs3FldXEPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7jxquilthnH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import ipykernel\n",
        "from notebook.notebookapp import list_running_servers\n",
        "import requests\n",
        "from urllib.parse import urljoin\n",
        "import json\n",
        "import os\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import glob\n",
        "import copy\n",
        "import logging\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the working directory to the path of the current notebook"
      ],
      "metadata": {
        "id": "OI0DpEd3_6rJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8mwIHFzdEk8",
        "outputId": "21d5b9a9-529b-4240-cb61-a3a42cdfc94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive to access files stored in it\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def get_notebook_path():\n",
        "    # Get the path of the current Jupyter notebook\n",
        "    kernel_id = re.search('kernel-(.*).json', ipykernel.get_connection_file()).group(1)\n",
        "    servers = list_running_servers()\n",
        "    for ss in servers:\n",
        "        response = requests.get(urljoin(ss['url'], 'api/sessions'), params={'token': ss.get('token', '')})\n",
        "        for nn in json.loads(response.text):\n",
        "            if nn['kernel']['id'] == kernel_id:\n",
        "                relative_path = nn['notebook']['path']\n",
        "                return os.path.join(ss['notebook_dir'], relative_path)\n",
        "\n",
        "def get_folder_path(folder_id):\n",
        "    # Recursively get the full path of a folder given its ID\n",
        "    if folder_id:\n",
        "        folder = drive_service.files().get(fileId=folder_id, fields=\"name, parents\").execute()\n",
        "        folder_name = folder.get('name')\n",
        "        parents = folder.get('parents')\n",
        "        if parents:\n",
        "            parent_path = get_folder_path(parents[0])\n",
        "            return parent_path + '/' + folder_name\n",
        "        else:\n",
        "            return folder_name\n",
        "    return ''\n",
        "\n",
        "def get_file_path(file_id):\n",
        "    # Recursively get the full path of a file given its ID\n",
        "    file = drive_service.files().get(fileId=file_id, fields=\"name, parents\").execute()\n",
        "    file_name = file.get('name')\n",
        "    parents = file.get('parents')\n",
        "    if parents:\n",
        "        parent_id = parents[0]\n",
        "        parent_path = get_folder_path(parent_id)\n",
        "        return parent_path\n",
        "    else:\n",
        "        return file_name\n",
        "\n",
        "# Get the path of the current notebook\n",
        "notebook_path = get_notebook_path()\n",
        "\n",
        "\n",
        "# Authenticate and initialize the Google Drive API\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Extract the file ID from the notebook path (assumes file ID is part of the path)\n",
        "file_id = re.search(r'fileId=(\\w+)', notebook_path).group(1)\n",
        "\n",
        "# Get the full path of the file using its ID\n",
        "file_path = get_file_path(file_id)\n",
        "if 'マイドライブ' in file_path:\n",
        "    converted_path = re.sub(r'(^|/)マイドライブ($|/)', '/content/drive/MyDrive/', file_path)\n",
        "elif 'MyDrive' in file_path:\n",
        "    converted_path = re.sub(r'(^|/)MyDrive($|/)', '/content/drive/MyDrive/', file_path)\n",
        "else:\n",
        "    converted_path = '/content/drive/MyDrive/' + file_path\n",
        "\n",
        "# Change the working directory to the converted path\n",
        "os.chdir(converted_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu8gmdiVLuXo"
      },
      "source": [
        "# Prepare data extracted by human"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teTsIJUB1w8W"
      },
      "outputs": [],
      "source": [
        "# Specify the file path and sheet name for the Excel file\n",
        "file_path = 'DE.xlsx'\n",
        "sheet_name = 'R1_FINAL'\n",
        "\n",
        "\n",
        "# Read the Excel file, specifying header=1 because the first row is blank\n",
        "df = pd.read_excel(file_path, sheet_name=sheet_name, header=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjBiHhil9hkG"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get the list of arm name"
      ],
      "metadata": {
        "id": "IfQcxRuXYP8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3I0BjnQ9hkI"
      },
      "outputs": [],
      "source": [
        "file_path = 'arm_matched_list.xlsx'\n",
        "sheet_name = 'Sheet1'\n",
        "\n",
        "df_arm_matched = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "df_arm_matched = df_arm_matched.drop(df.index[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_70K1d1n9hkI"
      },
      "outputs": [],
      "source": [
        "df_arm_matched_list = {}\n",
        "df_arm_matched_list[\"chat\"] = df_arm_matched"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create output storage dict"
      ],
      "metadata": {
        "id": "SQcgGAOiZ7Ad"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULNOhjH9hkJ"
      },
      "outputs": [],
      "source": [
        "# Define the path to the JSON file\n",
        "json_file_path = 'original_description_insomnia.json'\n",
        "\n",
        "# Load the JSON data from the file\n",
        "with open(json_file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract variables that are of numeric type (integer or number)\n",
        "numeric_fields = [field for field in data if field['type'] in ('integer', 'number')]\n",
        "numeric_list = [item['name'] for item in numeric_fields]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRxFNDEC9hkJ"
      },
      "source": [
        "## Calculate accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a45cdbb-31fd-4ddc-9d77-ff4f30fbce9a",
        "id": "iuNX9G969hkJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 0 0 done.\n",
            "5 0 0 done.\n",
            "5 0 1 done.\n",
            "5 0 1 done.\n",
            "5 0 2 done.\n",
            "5 0 2 done.\n",
            "5 0 2 done.\n",
            "5 0 3 done.\n",
            "5 0 3 done.\n",
            "5 0 4 done.\n",
            "5 0 4 done.\n",
            "5 1 0 done.\n",
            "5 1 0 done.\n",
            "5 1 1 done.\n",
            "5 1 1 done.\n",
            "5 1 2 done.\n",
            "5 1 2 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/1_fold/4/Kaplan2018/CBTI-BP with RISE-UP/RISE-UP --> excluded\n",
            "ERROR:root:5_paper/1_fold/4/Kaplan2018/Psychoeducation (PE)/PE --> excluded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 2 0 done.\n",
            "5 2 0 done.\n",
            "5 2 2 done.\n",
            "5 2 2 done.\n",
            "5 2 3 done.\n",
            "5 2 3 done.\n",
            "5 3 1 done.\n",
            "5 3 1 done.\n",
            "5 3 3 done.\n",
            "5 3 3 done.\n",
            "5 3 4 done.\n",
            "5 3 4 done.\n",
            "5 4 0 done.\n",
            "5 4 0 done.\n",
            "5 4 0 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/4_fold/1/Gehrman2021/in-person CBT-I/In-person --> no selected_rows: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 4 1 done.\n",
            "5 4 1 done.\n",
            "5 4 2 done.\n",
            "5 4 2 done.\n",
            "5 4 3 done.\n",
            "5 4 3 done.\n",
            "5 4 4 done.\n",
            "5 4 4 done.\n",
            "5 5 0 done.\n",
            "5 5 0 done.\n",
            "5 5 0 done.\n",
            "5 5 1 done.\n",
            "5 5 1 done.\n",
            "5 5 1 done.\n",
            "5 5 2 done.\n",
            "5 5 2 done.\n",
            "5 5 3 done.\n",
            "5 5 3 done.\n",
            "5 5 4 done.\n",
            "5 5 4 done.\n",
            "5 6 0 done.\n",
            "5 6 0 done.\n",
            "5 6 0 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/6_fold/2/Mottaghi2015/Music and Cognitive-Behavioral Therapy (MAT)/MAT --> excluded\n",
            "ERROR:root:5_paper/6_fold/2/Mottaghi2015/Cognitive-Behavioral Therapy (CBT)/CBT --> excluded\n",
            "ERROR:root:5_paper/6_fold/2/Mottaghi2015/Control/Control --> excluded\n",
            "ERROR:root:5_paper/6_fold/3/Belleville2007/Taper intervention alone/Taper --> excluded\n",
            "ERROR:root:5_paper/6_fold/3/Belleville2007/Taper intervention combined with self-help CBT/Combined --> excluded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 6 4 done.\n",
            "5 6 4 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/7_fold/2/Huberty2021/Calm app/Intervention --> excluded\n",
            "ERROR:root:5_paper/7_fold/2/Huberty2021/wait-list control/Wait-list --> excluded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 7 3 done.\n",
            "5 7 3 done.\n",
            "5 7 4 done.\n",
            "5 7 4 done.\n",
            "5 8 0 done.\n",
            "5 8 0 done.\n",
            "5 8 1 done.\n",
            "5 8 1 done.\n",
            "5 8 3 done.\n",
            "5 8 3 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/8_fold/4/Margolies2013/CBT-I with adjunctive IRT/CBTI --> excluded\n",
            "ERROR:root:5_paper/8_fold/4/Margolies2013/waitlist control/Waitlist --> excluded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 9 0 done.\n",
            "5 9 0 done.\n",
            "5 9 1 done.\n",
            "5 9 1 done.\n",
            "5 9 1 done.\n",
            "5 9 2 done.\n",
            "5 9 2 done.\n",
            "5 9 4 done.\n",
            "5 9 4 done.\n"
          ]
        }
      ],
      "source": [
        "# Initialize variables and dictionaries for storing results\n",
        "first_flag = True\n",
        "first_flag_analysis = True\n",
        "accuracy_output = {}\n",
        "sensitivity_output = {}\n",
        "specificity_output = {}\n",
        "\n",
        "mean_accuracy_dict = {}\n",
        "mean_sensitivity_dict = {}\n",
        "mean_specificity_dict = {}\n",
        "\n",
        "shot_name_list = [\"chat\"] # List of shot names\n",
        "\n",
        "# Loop through each shot name\n",
        "for shot_name_num in range(1):\n",
        "  shot_name = shot_name_list[shot_name_num]\n",
        "\n",
        "\n",
        "  df_arm_matched = df_arm_matched_list[shot_name]\n",
        "  # Basic logging configuration\n",
        "  logging.basicConfig(level=logging.ERROR, filename=f'error_log_{shot_name}.log', filemode='a',\n",
        "                      format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "  for n_index in range(5,6):\n",
        "    accuracy_output[n_index] = {}\n",
        "    sensitivity_output[n_index] = {}\n",
        "    specificity_output[n_index] = {}\n",
        "\n",
        "    accuracy_total = 0\n",
        "    sensitivity_total = 0\n",
        "    specificity_total = 0\n",
        "\n",
        "\n",
        "    for fold_index in range(10):\n",
        "      # Initialize counters for evaluation metrics\n",
        "      non_text_match_count = 0 #The number of data matched\n",
        "      non_text_total = 0 #The total number of data\n",
        "      judge_count_TP = 0 #The number of true positive\n",
        "      judge_count_FN = 0 #The number of false negative\n",
        "      judge_count_P = 0 #The total number of variable for which data exists\n",
        "      judge_count_FP = 0 #The number of false positive\n",
        "      judge_count_TN = 0 #The number of true negative\n",
        "      judge_count_N = 0 #TThe total number of variable for which data does not exist\n",
        "\n",
        "      for eval_index in range(5):\n",
        "        try:\n",
        "          # Filter the DataFrame to get the relevant rows\n",
        "          filtered_df = df_arm_matched[(df_arm_matched['n_index'].astype(int) == n_index) & (df_arm_matched['fold_index'].astype(int) == fold_index) & (df_arm_matched['eval_index'].astype(int) == eval_index)].copy()\n",
        "          filtered_df.fillna(\"\", inplace=True)\n",
        "          index_list = filtered_df.index\n",
        "          paper_name = filtered_df.loc[index_list[0], \"paper_name\"]\n",
        "\n",
        "          # Create a dictionary of arm names extracted by GPT and by human\n",
        "          arm_dict = {}\n",
        "          for i in range(5):\n",
        "            key = filtered_df.loc[index_list[0], f\"key_{i+1}\"]\n",
        "            value = filtered_df.loc[index_list[0], f\"value_{i+1}\"]\n",
        "            if (key != \"\") and (value != \"\"):\n",
        "              arm_dict[key] = value\n",
        "        except:\n",
        "          continue;\n",
        "\n",
        "        # Iterate over the arm dictionary\n",
        "        for key, value in arm_dict.items():\n",
        "          # Construct the path to the GPT-extracted data JSON file\n",
        "          GPT_arm_path = f\"extracted_data_log_GPT_v3/{shot_name}/{n_index}_paper/{fold_index}_fold/extracted_data_{eval_index}.json\"\n",
        "          with open(GPT_arm_path, 'r', encoding='utf-8') as file:\n",
        "            extracted_data = json.load(file)\n",
        "\n",
        "          GPT_arm = key # Extracted arm name by GPT\n",
        "          human_arm = value # Corresponding human arm name\n",
        "\n",
        "          try:\n",
        "            # Extract data for the GPT arm and filter numeric fields\n",
        "            _extracted_data_GPT = extracted_data[paper_name][GPT_arm]\n",
        "            extracted_data_GPT = {key: _extracted_data_GPT[key] for key in numeric_list if key in _extracted_data_GPT}\n",
        "\n",
        "          except Exception as e:\n",
        "            logging.error(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> no arm name: {e}\")\n",
        "            with open(f'error_log_{shot_name}.txt', 'a') as file:\n",
        "              file.write(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> no arm name: {e}\")\n",
        "            continue;\n",
        "\n",
        "          try:\n",
        "            # Select rows corresponding to the human arm and convert to dictionary\n",
        "            selected_rows = df[(df['study'] == paper_name) & (df['Arm'] == human_arm)].copy()\n",
        "            filtered_df.fillna(\"*\", inplace=True) # Replace NaN values with asterisks\n",
        "            # Create a dictionary from the selected rows\n",
        "            extracted_data_human =selected_rows.to_dict(orient='records')[-1]\n",
        "\n",
        "          except Exception as e:\n",
        "            logging.error(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> no selected_rows: {e}\")\n",
        "            with open(f'error_log_{shot_name}.txt', 'a') as file:\n",
        "              file.write(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> no selected_rows: {e}\")\n",
        "            continue;\n",
        "\n",
        "          # Summarize extracted human data to match the keys of GPT data\n",
        "          summarized_extracted_data_human = {key: extracted_data_human.get(key, None) for key in extracted_data_GPT.keys()}\n",
        "          answer_dict = {key: None for key in extracted_data_GPT}  # Initialize answer dictionary\n",
        "\n",
        "          # Exclude RCTs with \"EXCLUDE\" in the Year field\n",
        "          if \"EXCLUDE\" in str(summarized_extracted_data_human[\"Year\"]):\n",
        "            logging.error(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> excluded\")\n",
        "            with open(f'error_log_{shot_name}.txt', 'a') as file:\n",
        "              file.write(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> excluded\")\n",
        "            continue;\n",
        "\n",
        "          # Exclude RCTs with \"None\" in the Year field for GPT data\n",
        "          if extracted_data_GPT[\"Year\"] == \"None\":\n",
        "            logging.error(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> GPT hasn't extracted\")\n",
        "            with open(f'error_log_{shot_name}.txt', 'a') as file:\n",
        "              file.write(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> GPT hasn't extracted\")\n",
        "            continue;\n",
        "\n",
        "          # Analyze the scores for each key\n",
        "          for key_DE in extracted_data_GPT:\n",
        "            # Retrieve values from human and GPT data\n",
        "              value1 = summarized_extracted_data_human[key_DE]\n",
        "              value2 = extracted_data_GPT[key_DE]\n",
        "\n",
        "              # Handle missing or null values in human data\n",
        "              if (str(value1) == \"nan\") or (\"*\" in str(value1)) or (str(value1) == \"\") or (str(value1) == \"null\"):\n",
        "                value1 = -1\n",
        "\n",
        "              if key_DE == \"ICC_for_cRCT\":# Protocol sets this to 0.05, but human extraction often uses '*'\n",
        "                value2 = -1\n",
        "\n",
        "              if (value2 == \"-1\") or (str(value2) == \"null\"):\n",
        "                value2 = -1\n",
        "\n",
        "              # If the severity at endpoint is reported, the change of severity does not need to be extracted\n",
        "              if key_DE in [\"Severity_ch_mean\", \"Severity_ch_sd\", \"Severity_ch_n\"]:\n",
        "                sum_ep = 0\n",
        "                for _value in [extracted_data_GPT[_key] for _key in [\"Severity_ep_mean\",\t\"Severity_ep_sd\",\t\"Severity_ep_n\"]]:\n",
        "                    if (_value != -1) and (str(_value) != \"-1\") and (str(_value) != \"null\") :\n",
        "                      sum_ep += 1\n",
        "                if sum_ep == 3:\n",
        "                  answer_dict[key_DE] = \"NA\"\n",
        "                  continue;\n",
        "\n",
        "              if re.search(r\"Severity_ch_.*_long\", key_DE):\n",
        "                sum_ep = 0\n",
        "                for _value in [extracted_data_GPT[_key] for _key in [\"Severity_mean_long\", \"Severity_sd_long\", \"Severity_n_long\"]]:\n",
        "                    if (_value != -1) and (str(_value) != \"-1\") and (str(_value) != \"null\") :\n",
        "                      sum_ep += 1\n",
        "                if sum_ep == 3:\n",
        "                  answer_dict[key_DE] = \"NA\"\n",
        "                  continue;\n",
        "\n",
        "              # Count total data\n",
        "              non_text_total += 1\n",
        "\n",
        "              # Calculate true negatives, false positives, true positives, and false negatives\n",
        "              if value1 == -1:\n",
        "                judge_count_N += 1\n",
        "                if value2 == -1:\n",
        "                  judge_count_TN += 1\n",
        "                else:\n",
        "                  judge_count_FP += 1\n",
        "\n",
        "              if value1 != -1:\n",
        "                judge_count_P += 1\n",
        "                if value2 != -1:\n",
        "                  judge_count_TP += 1\n",
        "                else:\n",
        "                  judge_count_FN += 1\n",
        "\n",
        "              # Compare data and update the answer dictionary\n",
        "              if value1 == value2:\n",
        "                answer_dict[key_DE] = \"True\"\n",
        "                non_text_match_count += 1\n",
        "\n",
        "              elif ((isinstance(value1, int) or isinstance(value1, float))  and (isinstance(value2, int) or isinstance(value2, int))) and (round(value1*10) == round(value2*10)): #Rounding to the second decimal point to see if they match\n",
        "                answer_dict[key_DE] = \"True\"\n",
        "                non_text_match_count += 1\n",
        "              else:\n",
        "                answer_dict[key_DE] = \"False\"\n",
        "\n",
        "          # Combine extracted data from GPT and human, and the answer dictionary into a DataFrame\n",
        "          all_dicts = [extracted_data_GPT, summarized_extracted_data_human, answer_dict]\n",
        "          all_df = pd.DataFrame(all_dicts)\n",
        "          all_df.insert(0, 'arm_name', [GPT_arm, human_arm, \"\"])\n",
        "          all_df.insert(0, 'paper_name', [paper_name,\"\", \"\"])\n",
        "          all_df.insert(0, 'eval_index', [eval_index, \"\", \"\"])\n",
        "          all_df.insert(0, 'fold_index', [fold_index, \"\", \"\"])\n",
        "          all_df.insert(0, 'n_index', [n_index, \"\", \"\"])\n",
        "          all_df.insert(0, 'shot_name', [shot_name, \"\", \"\"])\n",
        "\n",
        "          # Add a row of NaN values to separate different evaluations\n",
        "          all_df.loc[len(all_df)] = [np.nan] * len(all_df.columns)\n",
        "\n",
        "          # Append the results to the output DataFrame\n",
        "          if first_flag:\n",
        "            output = all_df.copy()\n",
        "            first_flag = False\n",
        "          else:\n",
        "            output = pd.concat([output, all_df], ignore_index=True)\n",
        "          print(n_index,fold_index,eval_index, \"done.\")\n",
        "\n",
        "      # Calculate accuracy, sensitivity, and specificity for numerical variables\n",
        "      non_text_accuracy = non_text_match_count / non_text_total if non_text_total > 0 else 0\n",
        "      sensitivity = judge_count_TP / judge_count_P if judge_count_P > 0 else 0\n",
        "      specificity = judge_count_TN / judge_count_N if judge_count_N > 0 else 0\n",
        "\n",
        "      # Accumulate totals for averaging later\n",
        "      accuracy_total += non_text_accuracy\n",
        "      sensitivity_total += sensitivity\n",
        "      specificity_total += specificity\n",
        "\n",
        "      # Create DataFrames to store the results\n",
        "      accuracy_output_df = pd.DataFrame({\"match_count\": [non_text_match_count], \"text_total\": [non_text_total], \"accuracy\": [non_text_accuracy]})\n",
        "      sensitivity_output_df = pd.DataFrame({\"TP\": [judge_count_TP], \"total_P\": [judge_count_P], \"sensitivity\": [sensitivity]})\n",
        "      specificity_output_df = pd.DataFrame({\"TN\": [judge_count_TN], \"total_N\": [judge_count_N], \"specificity\": [specificity]})\n",
        "\n",
        "      # Combine the DataFrames horizontally\n",
        "      combined_df = pd.concat([accuracy_output_df, sensitivity_output_df, specificity_output_df], axis=1)\n",
        "\n",
        "      # Append the combined results to the output analysis DataFrame\n",
        "      if first_flag_analysis:\n",
        "        output_analysis = combined_df.copy()\n",
        "        first_flag_analysis = False\n",
        "      else:\n",
        "        output_analysis = pd.concat([output_analysis, combined_df], ignore_index=True)\n",
        "\n",
        "\n",
        "    accuracy_output_df = pd.DataFrame({\"match_count\": [n_index+1], \"text_total\": [\"\"], \"accuracy\": [\"\"]})\n",
        "    sensitivity_output_df = pd.DataFrame({\"TP\": [\"\"], \"total_P\": [\"\"], \"sensitivity\": [\"\"]})\n",
        "    specificity_output_df = pd.DataFrame({\"TN\": [\"\"], \"total_N\": [\"\"], \"specificity\": [\"\"]})\n",
        "\n",
        "    combined_df = pd.concat([accuracy_output_df, sensitivity_output_df, specificity_output_df], axis=1)\n",
        "\n",
        "    # Append the combined results to the output analysis DataFrame\n",
        "    output_analysis = pd.concat([output_analysis, combined_df], ignore_index=True)\n",
        "\n",
        "    # Calculate mean accuracy, sensitivity, and specificity\n",
        "    mean_accuracy = accuracy_total/10\n",
        "    mean_sensitivity = sensitivity_total/10\n",
        "    mean_specificity = specificity_total/10\n",
        "\n",
        "    # Store the mean values in dictionaries\n",
        "    mean_accuracy_dict[n_index] = {\"mean_accuracy\": mean_accuracy}\n",
        "    mean_sensitivity_dict[n_index] = {\"mean_sensitivity\": mean_sensitivity}\n",
        "    mean_specificity_dict[n_index] = {\"mean_specificity\": mean_specificity}\n",
        "\n",
        "# Create DataFrames from the mean dictionaries\n",
        "mean_accuracy_df = pd.DataFrame(mean_accuracy_dict)\n",
        "mean_sensitivity_df = pd.DataFrame(mean_sensitivity_dict)\n",
        "mean_specificity_df = pd.DataFrame(mean_specificity_dict)\n",
        "\n",
        "mean_combined_df = pd.concat([mean_accuracy_df, mean_sensitivity_df, mean_specificity_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYYEjxNs9hkN"
      },
      "outputs": [],
      "source": [
        "# Save the output, mean combined, and output analysis DataFrames to an Excel file\n",
        "with pd.ExcelWriter(f'output_analysis_{shot_name}.xlsx') as writer:\n",
        "    output.to_excel(writer, sheet_name='Sheet1', index=False) # Save the 'output' DataFrame to Sheet1\n",
        "    mean_combined_df.to_excel(writer, sheet_name='Sheet2', index=True) # Save the 'mean_combined_df' DataFrame to Sheet2\n",
        "    output_analysis.to_excel(writer, sheet_name='Sheet3', index=False) # Save the 'output_analysis' DataFrame to Sheet3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate sensitivity and specificity"
      ],
      "metadata": {
        "id": "DEtSdvIQ9fN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables and dictionaries for storing results\n",
        "first_flag = True\n",
        "first_flag_analysis = True\n",
        "accuracy_output = {}\n",
        "sensitivity_output = {}\n",
        "specificity_output = {}\n",
        "\n",
        "mean_accuracy_dict = {}\n",
        "mean_sensitivity_dict = {}\n",
        "mean_specificity_dict = {}\n",
        "\n",
        "# List of shot names\n",
        "shot_name_list = [\"chat\"]\n",
        "\n",
        "# Loop through each shot name\n",
        "for shot_name_num in range(1):\n",
        "  shot_name = shot_name_list[shot_name_num]\n",
        "\n",
        "\n",
        "  df_arm_matched = df_arm_matched_list[shot_name]\n",
        "  logging.basicConfig(level=logging.ERROR, filename=f'error_log_{shot_name}.log', filemode='a',\n",
        "                      format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "  for n_index in range(5,6):\n",
        "    accuracy_output[n_index] = {}\n",
        "    sensitivity_output[n_index] = {}\n",
        "    specificity_output[n_index] = {}\n",
        "\n",
        "    accuracy_total = 0\n",
        "    sensitivity_total = 0\n",
        "    specificity_total = 0\n",
        "\n",
        "\n",
        "    for fold_index in range(10):\n",
        "      # Initialize counters for evaluation metrics\n",
        "      non_text_match_count = 0 #The number of data matched\n",
        "      non_text_total = 0 #The total number of data\n",
        "      judge_count_TP = 0 #The number of true positive\n",
        "      judge_count_FN = 0 #The number of false negative\n",
        "      judge_count_P = 0 #The total number of variable for which data exists\n",
        "      judge_count_FP = 0 #The number of false positive\n",
        "      judge_count_TN = 0 #The number of true negative\n",
        "      judge_count_N = 0 #TThe total number of variable for which data does not exist\n",
        "\n",
        "      for eval_index in range(5):\n",
        "        try:\n",
        "          # Filter the DataFrame to get the relevant rows\n",
        "          filtered_df = df_arm_matched[(df_arm_matched['n_index'].astype(int) == n_index) & (df_arm_matched['fold_index'].astype(int) == fold_index) & (df_arm_matched['eval_index'].astype(int) == eval_index)].copy()\n",
        "          filtered_df.fillna(\"\", inplace=True)\n",
        "          index_list = filtered_df.index\n",
        "          paper_name = filtered_df.loc[index_list[0], \"paper_name\"]\n",
        "\n",
        "          # Create a dictionary of arm names extracted by GPT and by human\n",
        "          arm_dict = {}\n",
        "          for i in range(5):\n",
        "            key = filtered_df.loc[index_list[0], f\"key_{i+1}\"]\n",
        "            value = filtered_df.loc[index_list[0], f\"value_{i+1}\"]\n",
        "            if (key != \"\") and (value != \"\"):\n",
        "              arm_dict[key] = value\n",
        "        except:\n",
        "          continue;\n",
        "\n",
        "        # Iterate over the arm dictionary\n",
        "        for key, value in arm_dict.items():\n",
        "          GPT_arm_path = f\"extracted_data_log_GPT_v3/{shot_name}/{n_index}_paper/{fold_index}_fold/extracted_data_{eval_index}.json\"\n",
        "          with open(GPT_arm_path, 'r', encoding='utf-8') as file:\n",
        "            extracted_data = json.load(file)\n",
        "\n",
        "          GPT_arm = key\n",
        "          human_arm = value\n",
        "\n",
        "          try:\n",
        "            # Extract data for the GPT arm and filter numeric fields\n",
        "            _extracted_data_GPT = extracted_data[paper_name][GPT_arm]\n",
        "            extracted_data_GPT = {key: _extracted_data_GPT[key] for key in numeric_list if key in _extracted_data_GPT}\n",
        "\n",
        "          except Exception as e:\n",
        "            logging.error(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> no arm name: {e}\")\n",
        "            with open(f'error_log_{shot_name}.txt', 'a') as file:\n",
        "              file.write(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> no arm name: {e}\")\n",
        "            continue;\n",
        "\n",
        "\n",
        "          try:\n",
        "            # Select rows corresponding to the human arm and convert to dictionary\n",
        "            selected_rows = df[(df['study'] == paper_name) & (df['Arm'] == human_arm)].copy()\n",
        "            filtered_df.fillna(\"*\", inplace=True)\n",
        "            extracted_data_human =selected_rows.to_dict(orient='records')[-1]\n",
        "\n",
        "          except Exception as e:\n",
        "            logging.error(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> no selected_rows: {e}\")\n",
        "            with open(f'error_log_{shot_name}.txt', 'a') as file:\n",
        "              file.write(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> no selected_rows: {e}\")\n",
        "            continue;\n",
        "          summarized_extracted_data_human = {key: extracted_data_human.get(key, None) for key in extracted_data_GPT.keys()}\n",
        "          answer_dict = {key: None for key in extracted_data_GPT}\n",
        "\n",
        "          if \"EXCLUDE\" in str(summarized_extracted_data_human[\"Year\"]):\n",
        "            logging.error(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> excluded\")\n",
        "            with open(f'error_log_{shot_name}.txt', 'a') as file:\n",
        "              file.write(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> excluded\")\n",
        "            continue;\n",
        "\n",
        "          if extracted_data_GPT[\"Year\"] == \"None\":\n",
        "            logging.error(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> GPT hasn't extracted\")\n",
        "            with open(f'error_log_{shot_name}.txt', 'a') as file:\n",
        "              file.write(f\"{n_index}_paper/{fold_index}_fold/{eval_index}/{paper_name}/{GPT_arm}/{human_arm} --> GPT hasn't extracted\")\n",
        "            continue;\n",
        "\n",
        "\n",
        "          for key_DE in extracted_data_GPT:\n",
        "              value1 = summarized_extracted_data_human[key_DE]\n",
        "              value2 = extracted_data_GPT[key_DE]\n",
        "\n",
        "              # Handle missing or null values\n",
        "              if (str(value1) == \"nan\") or (\"*\" in str(value1)) or (str(value1) == \"\") or (str(value1) == \"null\"):\n",
        "                value1 = -1\n",
        "\n",
        "              if key_DE == \"ICC_for_cRCT\":\n",
        "                value2 = -1\n",
        "              if (value2 == \"-1\") or (str(value2) == \"null\"):\n",
        "                value2 = -1\n",
        "\n",
        "              if key_DE in [\"Severity_ch_mean\", \"Severity_ch_sd\", \"Severity_ch_n\"]:\n",
        "                sum_ep = 0\n",
        "                for _value in [extracted_data_GPT[_key] for _key in [\"Severity_ep_mean\",\t\"Severity_ep_sd\",\t\"Severity_ep_n\"]]:\n",
        "                    if (_value != -1) and (str(_value) != \"-1\") and (str(_value) != \"null\") :\n",
        "                      sum_ep += 1\n",
        "                if sum_ep == 3:\n",
        "                  answer_dict[key_DE] = \"NA\"\n",
        "                  continue;\n",
        "\n",
        "              if re.search(r\"Severity_ch_.*_long\", key_DE):\n",
        "                sum_ep = 0\n",
        "                for _value in [extracted_data_GPT[_key] for _key in [\"Severity_mean_long\", \"Severity_sd_long\", \"Severity_n_long\"]]:\n",
        "                    if (_value != -1) and (str(_value) != \"-1\") and (str(_value) != \"null\") :\n",
        "                      sum_ep += 1\n",
        "                if sum_ep == 3:\n",
        "                  answer_dict[key_DE] = \"NA\"\n",
        "                  continue;\n",
        "\n",
        "              #Store one of TN, FP, TP, or FN in answer_dict\n",
        "              if value1 == -1:\n",
        "                judge_count_N += 1\n",
        "                if value2 == -1:\n",
        "                  judge_count_TN += 1\n",
        "                  answer_dict[key_DE] = \"TN\"\n",
        "                  continue;\n",
        "\n",
        "                else:\n",
        "                  judge_count_FP += 1\n",
        "                  answer_dict[key_DE] = \"FP\"\n",
        "                  continue;\n",
        "\n",
        "              if value1 != -1:\n",
        "                judge_count_P += 1\n",
        "                if value2 != -1:\n",
        "                  judge_count_TP += 1\n",
        "                  answer_dict[key_DE] = \"TP\"\n",
        "                  continue;\n",
        "\n",
        "                else:\n",
        "                  judge_count_FN += 1\n",
        "                  answer_dict[key_DE] = \"FN\"\n",
        "                  continue;\n",
        "\n",
        "          # Combine extracted data from GPT and human, and the answer dictionary into a DataFrame\n",
        "          all_dicts = [extracted_data_GPT, summarized_extracted_data_human, answer_dict]\n",
        "          all_df = pd.DataFrame(all_dicts)\n",
        "          all_df.insert(0, 'arm_name', [GPT_arm, human_arm, \"\"])\n",
        "          all_df.insert(0, 'paper_name', [paper_name,\"\", \"\"])\n",
        "          all_df.insert(0, 'eval_index', [eval_index, \"\", \"\"])\n",
        "          all_df.insert(0, 'fold_index', [fold_index, \"\", \"\"])\n",
        "          all_df.insert(0, 'n_index', [n_index, \"\", \"\"])\n",
        "          all_df.insert(0, 'shot_name', [shot_name, \"\", \"\"])\n",
        "          all_df.loc[len(all_df)] = [np.nan] * len(all_df.columns)\n",
        "\n",
        "          # Append the results to the output DataFrame\n",
        "          if first_flag:\n",
        "            output = all_df.copy()\n",
        "            first_flag = False\n",
        "          else:\n",
        "            output = pd.concat([output, all_df], ignore_index=True)\n",
        "          print(n_index,fold_index,eval_index, \"done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW3sNBcA9ewA",
        "outputId": "b37cae6e-aeca-415f-b89f-5a2e710a8109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 0 0 done.\n",
            "5 0 0 done.\n",
            "5 0 1 done.\n",
            "5 0 1 done.\n",
            "5 0 2 done.\n",
            "5 0 2 done.\n",
            "5 0 2 done.\n",
            "5 0 3 done.\n",
            "5 0 3 done.\n",
            "5 0 4 done.\n",
            "5 0 4 done.\n",
            "5 1 0 done.\n",
            "5 1 0 done.\n",
            "5 1 1 done.\n",
            "5 1 1 done.\n",
            "5 1 2 done.\n",
            "5 1 2 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/1_fold/4/Kaplan2018/CBTI-BP with RISE-UP/RISE-UP --> excluded\n",
            "ERROR:root:5_paper/1_fold/4/Kaplan2018/Psychoeducation (PE)/PE --> excluded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 2 0 done.\n",
            "5 2 0 done.\n",
            "5 2 2 done.\n",
            "5 2 2 done.\n",
            "5 2 3 done.\n",
            "5 2 3 done.\n",
            "5 3 1 done.\n",
            "5 3 1 done.\n",
            "5 3 3 done.\n",
            "5 3 3 done.\n",
            "5 3 4 done.\n",
            "5 3 4 done.\n",
            "5 4 0 done.\n",
            "5 4 0 done.\n",
            "5 4 0 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/4_fold/1/Gehrman2021/in-person CBT-I/In-person --> no selected_rows: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 4 1 done.\n",
            "5 4 1 done.\n",
            "5 4 2 done.\n",
            "5 4 2 done.\n",
            "5 4 3 done.\n",
            "5 4 3 done.\n",
            "5 4 4 done.\n",
            "5 4 4 done.\n",
            "5 5 0 done.\n",
            "5 5 0 done.\n",
            "5 5 0 done.\n",
            "5 5 1 done.\n",
            "5 5 1 done.\n",
            "5 5 1 done.\n",
            "5 5 2 done.\n",
            "5 5 2 done.\n",
            "5 5 3 done.\n",
            "5 5 3 done.\n",
            "5 5 4 done.\n",
            "5 5 4 done.\n",
            "5 6 0 done.\n",
            "5 6 0 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/6_fold/2/Mottaghi2015/Music and Cognitive-Behavioral Therapy (MAT)/MAT --> excluded\n",
            "ERROR:root:5_paper/6_fold/2/Mottaghi2015/Cognitive-Behavioral Therapy (CBT)/CBT --> excluded\n",
            "ERROR:root:5_paper/6_fold/2/Mottaghi2015/Control/Control --> excluded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 6 0 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/6_fold/3/Belleville2007/Taper intervention alone/Taper --> excluded\n",
            "ERROR:root:5_paper/6_fold/3/Belleville2007/Taper intervention combined with self-help CBT/Combined --> excluded\n",
            "ERROR:root:5_paper/7_fold/2/Huberty2021/Calm app/Intervention --> excluded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 6 4 done.\n",
            "5 6 4 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/7_fold/2/Huberty2021/wait-list control/Wait-list --> excluded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 7 3 done.\n",
            "5 7 3 done.\n",
            "5 7 4 done.\n",
            "5 7 4 done.\n",
            "5 8 0 done.\n",
            "5 8 0 done.\n",
            "5 8 1 done.\n",
            "5 8 1 done.\n",
            "5 8 3 done.\n",
            "5 8 3 done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:5_paper/8_fold/4/Margolies2013/CBT-I with adjunctive IRT/CBTI --> excluded\n",
            "ERROR:root:5_paper/8_fold/4/Margolies2013/waitlist control/Waitlist --> excluded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 9 0 done.\n",
            "5 9 0 done.\n",
            "5 9 1 done.\n",
            "5 9 1 done.\n",
            "5 9 1 done.\n",
            "5 9 2 done.\n",
            "5 9 2 done.\n",
            "5 9 4 done.\n",
            "5 9 4 done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the output DataFrames to an Excel file\n",
        "with pd.ExcelWriter(f'output_analysis_{shot_name}_sensitivity_specificity.xlsx') as writer:\n",
        "    output.to_excel(writer, sheet_name='Sheet1', index=False)"
      ],
      "metadata": {
        "id": "URjR2cTz9hDg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgNvYINYiuHe1evFWhki6H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}